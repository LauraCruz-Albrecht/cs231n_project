{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as local_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import pytorch_utils\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL\n",
    "from importlib import reload\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils, models\n",
    "import os, sys\n",
    "from random import shuffle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "IMG_SZ = 224\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(DataLoader.Dataset):\n",
    "\n",
    "    def __init__(self, src_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_folder (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.filenames = [f for f in os.listdir(src_folder) if os.path.isfile(os.path.join(src_folder, f)) \n",
    "                          and f != '.DS_Store']\n",
    "        self.src_folder = src_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.src_folder, self.filenames[idx])\n",
    "        \n",
    "        x = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        imgf = self.filenames[idx]\n",
    "        y = int(imgf[imgf.index('_') + 1 : imgf.index('.')]) # filename format: [id_label.jpg]\n",
    "        sample = (x, y)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(directory, batch_size):\n",
    "    '''\n",
    "    takes in directory for train and val data, and returns loaders for both\n",
    "    applies normalization:\n",
    "      1. convert values to range 0-1\n",
    "      2. set mean, std to those specified in pytorch pretrained models (https://pytorch.org/docs/master/torchvision/models.html)\n",
    "    \n",
    "    usage:\n",
    "        loader_train = get_loader(train_directory, batch_sz)\n",
    "        loader_val = get_loader(val_directory, batch_sz)\n",
    "    '''\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                      std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "       transforms.ToTensor(),  # converts to range 0-1\n",
    "       normalize               # sets mean, std\n",
    "    ])\n",
    "    \n",
    "    dset = LandmarksDataset(directory, transform=preprocess)\n",
    "    loader = DataLoader.DataLoader(dataset=dset, batch_size=batch_size)\n",
    "    \n",
    "    print ('dataset size', len(dset))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and accuracy checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for t, (x, y) in enumerate(loader):\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader_train, epochs=1, stop=1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    iters = []\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        print ('epoch', e)\n",
    "        \n",
    "        num_iters = len(loader_train)\n",
    "        want_print = 10\n",
    "        print_every = 50\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "        \n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print(' Iteration %d out of %d, loss = %.4f' % (t, num_iters, loss.item()))\n",
    "                iters.append(t)\n",
    "                losses.append(loss.item())\n",
    "            \n",
    "            # break early if we only want to use a part of the dataset (for hyperparameter tuning)\n",
    "            if t > stop * num_iters:\n",
    "                break\n",
    "\n",
    "    return iters, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_data(plotting_data, title_str):\n",
    "    '''\n",
    "    Plots all loss curves for hyperparameter tuning runs.\n",
    "    Plots loss vs. iterations.\n",
    "    '''\n",
    "    for plot_data in plotting_data:\n",
    "        iters, losses, label = plot_data        \n",
    "        plt.plot(iters, losses, label=label)\n",
    "    plt.title(title_str)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=(1.1, 0.25))\n",
    "    \n",
    "    # set figsize\n",
    "    plt.rcParams[\"figure.figsize\"] = (9, 6)\n",
    "    \n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_data_v2(plotting_data, title_str):\n",
    "    '''\n",
    "    Plots all loss curves for hyperparameter tuning runs\n",
    "    This version plots loss vs. percent of iterations complete. This means\n",
    "    each run has the same x distance on the graph.\n",
    "    '''\n",
    "    max_n = max([len(iters) for iters, _, _ in plotting_data])\n",
    "    \n",
    "    for plot_data in plotting_data:\n",
    "        iters, losses, label = plot_data \n",
    "        n = len(losses)\n",
    "        \n",
    "        # need to go between 0 and max_n, with n entries\n",
    "        dist = (max_n - 1) / (n-1)\n",
    "        x = [e * dist for e in range(n)]\n",
    "        x_max = max(x)\n",
    "        x = [e / x_max for e in x] # normalize to 0 - 1\n",
    "                \n",
    "        plt.plot(x, losses, label=label)\n",
    "        \n",
    "    plt.title(title_str)\n",
    "    plt.xlabel('Fraction of tuning run elapsed')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=(1.1, 0.25))\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (9, 6) # set figsize\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_loss(iters, losses, title_str):\n",
    "    plt.plot(iters, losses)\n",
    "    plt.title(title_str)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # set figsize\n",
    "    plt.rcParams[\"figure.figsize\"] = (9, 6)\n",
    "    \n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 30395\n",
      "epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9f25ecc6c33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stopping early to save time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lr: %f, batch sz: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-374feb2e3338>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loader_train, epochs, stop)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mthreshold\u001b[0;34m(input, threshold, value, inplace)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \"\"\"\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# referenced https://www.kaggle.com/gntoni/using-pytorch-resnet\n",
    "\n",
    "# hyperparam tuning on lr and batch sz\n",
    "lr_vals = [0.0001, 0.001, 0.01]\n",
    "# batch_sizes = [10, 20, 50, 80]\n",
    "batch_sizes = [80]\n",
    "\n",
    "train_directory = '../data/data_200c/train'\n",
    "val_directory = '../data/data_200c/val'\n",
    "\n",
    "momentum = 0.9\n",
    "num_classes = 200\n",
    "\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "plotting_data = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    loader_train = get_loader(train_directory, batch_size)\n",
    "    for lr in lr_vals:\n",
    "        # set up resnet model with custom FC layer to predict our number of classes\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "        num_epochs = 1\n",
    "        iters, losses = train(model, optimizer, loader_train, num_epochs, 0.20) # stopping early to save time\n",
    "        \n",
    "        s = 'lr: %f, batch sz: %d' % (lr, batch_size)\n",
    "        plotting_data.append((iters, losses, s))\n",
    "        \n",
    "        final_loss = losses[-1]\n",
    "        print('got a loss of ', final_loss, ' for parameters ', 'batch size: ', batch_size, ' lr: ', lr)\n",
    "        if final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            best_model = model\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch_size\n",
    "            \n",
    "print(\"best values \", best_loss, best_lr, best_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best_lr, best_batch_size\n",
    "lr = best_lr\n",
    "batch_size = best_batch_size\n",
    "\n",
    "# load data\n",
    "loader_val = get_loader(val_directory, batch_size)\n",
    "loader_train = get_loader(train_directory, batch_size)\n",
    "\n",
    "# set up squeezenet model with custom final conv layer to predict our number of classes\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# run for full epoch\n",
    "num_epochs = 1\n",
    "iters, losses = train(model, optimizer, loader_train, num_epochs)\n",
    "\n",
    "# get final accuracies\n",
    "print ('validation accuracy is ', check_accuracy(loader_val, model))\n",
    "print ('training accuracy is ', check_accuracy(loader_train, model))\n",
    "\n",
    "# plot loss for fully trained best model\n",
    "plot_single_loss(iters, losses, 'Loss for tuned Resnet18 model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(plotting_data, title_str=\"Hyperparameter tuning for Resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_data_v2(plotting_data, title_str=\"Hyperparameter tuning for Resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL NET!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 30395\n",
      "epoch 0\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      " Iteration 0 out of 3040, loss = 5.2462\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      " Iteration 5 out of 3040, loss = 5.2405\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n",
      "Finished an inception block\n",
      "Finished a DL block\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "    \n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "    \n",
    "class InceptionModule(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 32, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 32, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, out_channels-3*32, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "#         print(\"Finished an inception block\")\n",
    "        return torch.cat(outputs, 1)    \n",
    "    \n",
    "class DLNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filter_size=3, padding=1, pool_size=2):\n",
    "        super(DLNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, filter_size, padding=padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, filter_size, padding=padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(pool_size)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        \n",
    "        out = self.relu2(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "#         print(\"Finished a DL block\")\n",
    "        return out\n",
    "\n",
    "\n",
    "num_hidden = 400\n",
    "momentum = 0.9\n",
    "num_classes = 200\n",
    "\n",
    "# hyperparam tuning on lr and batch sz\n",
    "lr_vals = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [10, 20, 50, 80]\n",
    "\n",
    "train_directory = '../data/data_200c/train'\n",
    "val_directory = '../data/data_200c/val'\n",
    "\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "plotting_data = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    loader_train = get_loader(train_directory, batch_size)\n",
    "    for lr in lr_vals:\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            DLNetBlock(64, 64),\n",
    "            nn.Dropout(),\n",
    "            InceptionModule(64, 128),\n",
    "            nn.Dropout(),\n",
    "            DLNetBlock(128, 128),\n",
    "            nn.Dropout(),\n",
    "            InceptionModule(128, 256),\n",
    "            nn.Dropout(),\n",
    "            DLNetBlock(256, 256),\n",
    "\n",
    "            # added these layers here so that the following linear layer does not have too many parameters and crash\n",
    "            nn.Conv2d(256, 3, 3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            Flatten(),\n",
    "            nn.Linear(3 * 14 * 14, num_hidden),\n",
    "            nn.Linear(num_hidden, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        \n",
    "        num_epochs = 1\n",
    "        iters, losses = train(model, optimizer, loader_train, num_epochs, 0.20) # stopping early to save time\n",
    "        \n",
    "        s = 'lr: %f, batch sz: %d' % (lr, batch_size)\n",
    "        plotting_data.append((iters, losses, s))\n",
    "        \n",
    "        final_loss = losses[-1]\n",
    "        print('got a loss of ', final_loss, ' for parameters ', 'batch size: ', batch_size, ' lr: ', lr)\n",
    "        if final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            best_model = model\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch_size\n",
    "            \n",
    "print(\"best values \", best_loss, best_lr, best_batch_size)\n",
    "# get final accuracies\n",
    "print ('validation accuracy is ', check_accuracy(loader_val, best_model))\n",
    "print ('training accuracy is ', check_accuracy(loader_train, best_model))\n",
    "\n",
    "# plot loss for fully trained best model\n",
    "plot_single_loss(iters, losses, 'Loss for tuned custom model')\n",
    "plot_loss_data_v2(plotting_data, title_str=\"Hyperparameter tuning for Custom Net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparam tuning on lr and batch sz\n",
    "lr_vals = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [10, 20, 50, 80]\n",
    "\n",
    "train_directory = '../data/data_200c/train'\n",
    "val_directory = '../data/data_200c/val'\n",
    "\n",
    "# for testing\n",
    "# train_directory = '../data/data_200c/mini'\n",
    "# val_directory = '../data/data_200c/mini'\n",
    "# batch_sizes = [1, 2, 5, 8]\n",
    "\n",
    "momentum = 0.9\n",
    "num_classes = 200\n",
    "\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "plotting_data = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    loader_train = get_loader(train_directory, batch_size)\n",
    "    for lr in lr_vals:\n",
    "        # set up squeezenet model with custom final conv layer to predict our number of classes\n",
    "        model = models.squeezenet1_0(pretrained=True)\n",
    "        final_conv = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        model.num_classes = num_classes\n",
    "        model.classifier = nn.Sequential(\n",
    "                    nn.Dropout(p=0.5),\n",
    "                    final_conv,\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.AvgPool2d(13))\n",
    "        \n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "        num_epochs = 1\n",
    "        iters, losses = train(model, optimizer, loader_train, num_epochs, 0.20) # stopping early to save time\n",
    "        \n",
    "        s = 'lr: %f, batch sz: %d' % (lr, batch_size)\n",
    "        plotting_data.append((iters, losses, s))\n",
    "        \n",
    "        final_loss = losses[-1]\n",
    "        print('got a loss of ', final_loss, ' for parameters ', 'batch size: ', batch_size, ' lr: ', lr)\n",
    "        if final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            best_model = model\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch_size\n",
    "print (' ')          \n",
    "print(\"best loss: \", best_loss, 'best lr', best_lr, 'best batch sz', best_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss curves for hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_data(plotting_data, title_str=\"Hyperparameter tuning for Squeezenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_data_v2(plotting_data, title_str=\"Hyperparameter tuning for Squeezenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train best model for full epoch, get final val & train accuracy, plot loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best_lr, best_batch_size\n",
    "lr = best_lr\n",
    "batch_size = best_batch_size\n",
    "\n",
    "# load data\n",
    "loader_val = get_loader(val_directory, batch_size)\n",
    "loader_train = get_loader(train_directory, batch_size)\n",
    "\n",
    "# set up squeezenet model with custom final conv layer to predict our number of classes\n",
    "model = models.squeezenet1_0(pretrained=True)\n",
    "final_conv = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "model.num_classes = num_classes\n",
    "model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            final_conv,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(13))\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# run for full epoch\n",
    "num_epochs = 1\n",
    "iters, losses = train(model, optimizer, loader_train, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final accuracies\n",
    "print ('validation accuracy is ', check_accuracy(loader_val, model))\n",
    "print ('training accuracy is ', check_accuracy(loader_train, model))\n",
    "\n",
    "# plot loss for fully trained best model\n",
    "plot_single_loss(iters, losses, 'Loss for tuned Squeezenet model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss for fully trained best model\n",
    "plot_single_loss(iters, losses, 'Loss for tuned Squeezenet model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparam tuning on lr and batch sz\n",
    "lr_vals = [0.0001, 0.001, 0.01]\n",
    "batch_sizes = [10, 20, 50, 80]\n",
    "\n",
    "train_directory = '../data/data_200c/train'\n",
    "val_directory = '../data/data_200c/val'\n",
    "\n",
    "# for testing\n",
    "# train_directory = '../data/data_200c/mini'\n",
    "# val_directory = '../data/data_200c/mini'\n",
    "# batch_sizes = [1, 2, 5, 8]\n",
    "\n",
    "momentum = 0.9\n",
    "num_classes = 200\n",
    "\n",
    "best_lr = None\n",
    "best_batch_size = None\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "plotting_data = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    loader_train = get_loader(train_directory, batch_size)\n",
    "    for lr in lr_vals:\n",
    "        # set up inception model with custom final FC layer to predict our number of classes\n",
    "        model = models.inception_v3(pretrained=True)\n",
    "        model.aux_logit=False\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "        num_epochs = 1\n",
    "        iters, losses = train(model, optimizer, loader_train, num_epochs, 0.20) # stopping early to save time\n",
    "        \n",
    "        s = 'lr: %f, batch sz: %d' % (lr, batch_size)\n",
    "        plotting_data.append((iters, losses, s))\n",
    "        \n",
    "        final_loss = losses[-1]\n",
    "        print('got a loss of ', final_loss, ' for parameters ', 'batch size: ', batch_size, ' lr: ', lr)\n",
    "        if final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            best_model = model\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch_size\n",
    "print (' ')          \n",
    "print(\"best loss: \", best_loss, 'best lr', best_lr, 'best batch sz', best_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss curves for hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_data(plotting_data, title_str=\"Hyperparameter tuning for Inception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_data_v2(plotting_data, title_str=\"Hyperparameter tuning for Inception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train best model for full epoch, get final val & train accuracy, plot loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best_lr, best_batch_size\n",
    "lr = best_lr\n",
    "batch_size = best_batch_size\n",
    "\n",
    "# load data\n",
    "loader_val = get_loader(val_directory, batch_size)\n",
    "loader_train = get_loader(train_directory, batch_size)\n",
    "\n",
    "# set up inception model with custom final FC layer to predict our number of classes\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.aux_logit=False\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# run for full epoch\n",
    "num_epochs = 1\n",
    "iters, losses = train(model, optimizer, loader_train, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final accuracies\n",
    "print ('validation accuracy is ', check_accuracy(loader_val, model))\n",
    "print ('training accuracy is ', check_accuracy(loader_train, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss for fully trained best model\n",
    "plot_single_loss(iters, losses, 'Loss for tuned Inception model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
